{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries for data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Importing libraries for system management\n",
    "import os\n",
    "\n",
    "#Distance computation\n",
    "from pyjarowinkler import distance\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Check if Danish computers have the same problem...\n",
    "I_want_to_run_all_the_preprocessing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the current format (ISO-8859-14) into UTF-8 \n",
    "iconv -f ISO-8859-14 file_in.csv > file_out.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if I_want_to_run_all_the_preprocessing:\n",
    "    \n",
    "    #Copying into a new directory\n",
    "    ! cp -R ../../city_dump/data/LL/ ../../city_dump/data/utf8/\n",
    "\n",
    "    #Getting all files\n",
    "    d_path = '../../city_dump/data/utf8/'\n",
    "    l = next(os.walk(d_path))[2]\n",
    "\n",
    "    #Converting the files to utf8 (not sure this will work in computers different than mac...)\n",
    "    for file in l:\n",
    "        f_path = d_path+file\n",
    "        tmp_path = d_path+'tmp.csv'\n",
    "        ! iconv -f ISO-8859-14 $f_path > $tmp_path\n",
    "        !mv $tmp_path $f_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading DigDag\n",
    "and transforming it so I encode as many logical variation of the names as possible. In this way the variations for each place is only appied once whereas if we do the same for the data it will need to be applied to each place taking much longer. But the way to remember the original variation is to keep the \"enhedid\", which is the reference id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function adds extra rows in the dataframe *dd* for the \n",
    "# places where *word* exists and replaces it with *replacement*\n",
    "def adding_extra_rows(dd, word, replacement):\n",
    "    dd['special'] = dd['simplename'].apply(lambda x: word in x) \n",
    "    dd['simplename2'] = dd.simplename.apply(lambda x: x.replace(word,replacement))\n",
    "    return pd.concat([dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename']],dd[dd['special']][['navn', 'enhedid', 'enhedtype', 'art', 'simplename2']].rename(columns={'simplename2':'simplename'})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The digdag seems to have duplicates which I do not understand why\n",
    "dd = pd.read_csv('../../city_dump/data/rl_places_digdag_v1.txt', sep = '\\t', encoding='utf-16', dtype=str)\n",
    "dd_org = dd.copy()\n",
    "\n",
    "#Getting the Købstad and putting it to the original data\n",
    "dd2 = pd.read_csv('../../city_dump/data/koebstad.csv', sep = ';', encoding='utf-8', dtype=str)\n",
    "dd2['enhedtype'] = dd2['art'] \n",
    "dd = pd.concat([dd, dd2[~dd2.isna()][['navn','enhedid','enhedtype','art','simplename']]])\n",
    "dd['simplename'] = dd['simplename'].astype(str)\n",
    "\n",
    "\n",
    "#Moving the Købstad to the right column\n",
    "dd['art'] = np.where(dd['enhedtype'] == 'Købstad', ['Købstadskommune']*len(dd), dd['art'])\n",
    "\n",
    "#adding a conversion of special characters to latin letters (å -> aa, ø -> oe, æ -> ae) and adding them to the reference list\n",
    "dd = adding_extra_rows(dd, 'å', 'aa')\n",
    "dd = adding_extra_rows(dd, 'ø', 'oe')\n",
    "dd = adding_extra_rows(dd, 'æ', 'ae')\n",
    "\n",
    "#Preparing the names to append them at the simple name\n",
    "dd['art'] = dd['art'].apply({'Amt':'amt', 'Sogn':'sogn','Købstadskommune':'købstad', 'Geografisk Herred':'herred', 'Processing':''}.get)\n",
    "\n",
    "#Adding a duplicated list of their unit type to increase matches\n",
    "dd['simplename2'] = dd.apply(lambda row: str(row['simplename'])+' '+str(row['art']), axis=1)\n",
    "dd = pd.concat([dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename']],dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename2']].rename(columns={'simplename2':'simplename'})])\n",
    "\n",
    "#Adding the plural købstæder as well\n",
    "dd = adding_extra_rows(dd, 'købstad', 'købstæder')\n",
    "\n",
    "#Adding spaces instead of hyphens (new rows)\n",
    "dd = adding_extra_rows(dd, '-', ' ')\n",
    "\n",
    "#Removing spaces (new rows)\n",
    "dd = adding_extra_rows(dd, ' ', '')\n",
    "\n",
    "#Adding bysogn and landsogn (new rows)\n",
    "dd = adding_extra_rows(dd, 'sogn', 'bysogn')\n",
    "dd = adding_extra_rows(dd, 'sogn', 'landsogn')\n",
    "\n",
    "#adding rows where the last letter's \"e\" will be removed\n",
    "dd['special'] = dd['simplename'].str[-1] == 'e'\n",
    "dd['simplename2'] = dd['simplename'].str[:-1]\n",
    "dd = pd.concat([dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename']],dd[dd['special']][['navn', 'enhedid', 'enhedtype', 'art', 'simplename2']].rename(columns={'simplename2':'simplename'})])\n",
    "\n",
    "#Dropping duplicates from digdag\n",
    "#print(dd[dd.duplicated('simplename', keep=False)].sort_values('simplename').head(30))\n",
    "dd.drop_duplicates(['art','simplename'], keep='first', inplace=True)\n",
    "\n",
    "#Drop 'Non' from the list, this may be one of my artifacts...\n",
    "dd = dd[(dd.simplename.apply(lambda x: not 'Non' in x))]\n",
    "\n",
    "# TODO take into consideration that the same city name can be in varios geographical areas...\n",
    "# It should be matched with herred to increase accuracy for each record. However, right now we're only cleaning\n",
    "# not sure how pressing the issue is...\n",
    "nr_sogn = len(dd_org[dd_org.art=='Sogn'].simplename.unique()) \n",
    "dd.head(20)\n",
    "\n",
    "#TODO: Right now I can map things to amt for example which should not be the case...\n",
    "#dd =dd[dd.art='sogn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length original:\\t', len(dd_org), '\\nlength modified:\\t', len(dd), '\\nunique sogn keys:\\t', nr_sogn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the df from the $ separated file (for some reason pandas has problems with random lines)\n",
    "def get_df(f_path):\n",
    "    r= []\n",
    "    columns = []\n",
    "    with open(f_path) as f:\n",
    "        first = True\n",
    "        for line in f:\n",
    "            line = line.rstrip().split('$')\n",
    "            if first:\n",
    "                length = len(line)\n",
    "                columns = line\n",
    "                first=False\n",
    "            else:\n",
    "                r.append(line[:length])\n",
    "                \n",
    "    return pd.DataFrame(data=r, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name cleaning function\n",
    "def name_cleaner(s):\n",
    "    try:\n",
    "        o =s[working_column].lower().rstrip().replace('  ', ' ')#.replace(' købstad', '')\n",
    "        if ' (' in o: return o.split(' (')[0]\n",
    "        return o\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping all 1901 files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if I_want_to_run_all_the_preprocessing:\n",
    "    \n",
    "    l =['ft1901_LL_aalborg.txt',\n",
    "     'ft1901_LL_aarhus.txt',\n",
    "     'ft1901_LL_bornholm.txt',\n",
    "     'ft1901_LL_frederiksborg.txt',\n",
    "     'ft1901_LL_hjoerring.txt',\n",
    "     'ft1901_LL_holbaek.txt',\n",
    "     'ft1901_LL_kbhv.txt',\n",
    "     'ft1901_LL_maribo.txt',\n",
    "     'ft1901_LL_odense.txt',\n",
    "     'ft1901_LL_praestoe.txt',\n",
    "     'ft1901_LL_randers.txt',\n",
    "     'ft1901_LL_ribe.txt',\n",
    "     'ft1901_LL_ringkoebing.txt',\n",
    "     'ft1901_LL_roskilde.txt',\n",
    "     'ft1901_LL_skanderborg.txt',\n",
    "     'ft1901_LL_soroe.txt',\n",
    "     'ft1901_LL_svendborg.txt',\n",
    "     'ft1901_LL_thisted.txt',\n",
    "     'ft1901_LL_vejle.txt',\n",
    "     'ft1901_LL_viborg.txt']\n",
    "\n",
    "    _path = '../../city_dump/data/utf8/'\n",
    "\n",
    "    df_list = []\n",
    "    for f in l:\n",
    "\n",
    "        print(f)\n",
    "        #Loading the data\n",
    "        df_list.append(get_df(_path+f))\n",
    "\n",
    "    #Concatenating all the files\n",
    "    df_list = pd.concat(df_list, sort=False)\n",
    "\n",
    "    #Saving the unique file\n",
    "    df_list.to_csv(_path+'ft1901_LL.txt', sep='$', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files to work (with the 1901 collapsed)\n",
    "working_data = ['ft1845_LL.txt',\n",
    " 'ft1850_LL.txt',\n",
    " 'ft1860_LL.txt',\n",
    " 'ft1880_LL.txt',\n",
    " 'ft1885_LL.txt',\n",
    " 'ft1901_LL.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic matching\n",
    "(only performs the match if the names are identical)\n",
    "\n",
    "_This could be done only once by countring reference id and year. Then with the names we can keep constraining with matches_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_path = '../../city_dump/data/utf8/'\n",
    "\n",
    "#This is where I save the counts and then group it together\n",
    "out_list = []\n",
    "\n",
    "#where I save the missmatches and file year\n",
    "missmatches = []\n",
    "total_records = 0\n",
    "\n",
    "for f in working_data:\n",
    "    \n",
    "    print(f)\n",
    "    #Loading the data\n",
    "    df = get_df(_path+f)\n",
    "    \n",
    "    #Dropping empty rows due to the \"mal-conversion\" to utf8... The empty rows do contain data originally\n",
    "    #TODO: Fix the conversion thing (ask Barbara to provide me the UTF-8...)\n",
    "    df = df[~df.Herred.isna()]\n",
    "    print(len(df))\n",
    "    total_records = total_records + len(df)\n",
    "    \n",
    "    #Preforming the name cleaning for Sogne, Herred, and Amt\n",
    "    for working_column in ['Sogne','Herred','Amt']:\n",
    "        df[working_column] = df.apply(lambda x: name_cleaner(x), axis=1)\n",
    "\n",
    "    # Focusing on Sogn that match the Sogn list in DigDag\n",
    "    out = df.merge(dd[dd.art=='sogn'][['simplename', 'art', 'enhedid']], left_on='Sogne', right_on='simplename', how='left')\n",
    "\n",
    "    #keeping the missmatch\n",
    "    miss = out[out.art.isna()].drop_duplicates('Sogne', keep='first')\n",
    "    miss['year'] = f[2:6]\n",
    "    missmatches.append(miss)\n",
    "    \n",
    "    #Counting the matches\n",
    "    out = out[~out.art.isna()].groupby('enhedid').size().reset_index(name='counts')\n",
    "    \n",
    "    # setting the year\n",
    "    out['year'] = f[2:6]\n",
    "    \n",
    "    #Keeping the counts on RAM\n",
    "    out_list.append(out)\n",
    "    \n",
    "\n",
    "#Concatenating all the files (the counts and missmatches)\n",
    "out_list = pd.concat(out_list, sort=False)\n",
    "missmatches = pd.concat(missmatches, sort=False)\n",
    "\n",
    "#Saving the unique file in the HD\n",
    "out_list = out_list.merge(dd_org, on ='enhedid').pivot('enhedid', 'year', 'counts').fillna(0).astype(int)\n",
    "out_list.to_csv(_path+'../out/counts.txt', sep='$')\n",
    "out_list.to_csv(_path+'../out/places_FT_uniquevalues_01.tsv', sep='\\t')\n",
    "\n",
    "print('Done :D')\n",
    "\n",
    "nr_matched_records = out_list.sum().sum() #Two times sum, first one for year, and second one among years\n",
    "print('Matched records:',nr_matched_records, 'out of', total_records, '(', nr_matched_records/total_records*100,')%')\n",
    "print('Matched places:', len(out_list), 'out of', nr_sogn, '(', len(out_list)/nr_sogn*100,')%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cities that could not be matched on the first round counts per year\n",
    "missmatches.groupby('year').size().reset_index(name='counts').sort_values('counts', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing city similarity for all missmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the potential first matches\n",
    "\n",
    "mm = missmatches.Sogne.unique()\n",
    "sn = dd.simplename.unique() #Another possibility wowuld be to match it to the original (shorter)\n",
    "\n",
    "distance_jaro = np.zeros((len(sn),len(mm)))\n",
    "distance_leven = np.zeros((len(sn),len(mm)))\n",
    "\n",
    "for i in range(len(sn)):\n",
    "    if i%100 == 0: print(i, 'out of' , len(sn))\n",
    "        \n",
    "    for j in range(len(mm)): #The internal loop could be done using apply (it should be the longer one)\n",
    "        #If variable is none skip\n",
    "        if not mm[j]: continue\n",
    "        \n",
    "        #This matrix is not simetric because x and y axis are not the same!\n",
    "        try:\n",
    "            distance_jaro[i][j] = distance.get_jaro_distance(sn[i],mm[j])\n",
    "            distance_leven[i][j] = Levenshtein.distance(sn[i],mm[j])/(len(sn[i])+len(mm[j]))\n",
    "        except:\n",
    "            print('Could not make it',i,j)\n",
    "            print(sn[i],mm[j])\n",
    "        \n",
    "#Index is the reference names found in the data, columns to the digdag\n",
    "distance_jaro = pd.DataFrame(data = distance_jaro, columns = mm, index = sn)\n",
    "distance_leven = pd.DataFrame(data = distance_leven, columns = mm, index = sn)\n",
    "print(distance_jaro.shape) #(26428, 1237)\n",
    "distance_jaro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_org[dd_org.simplename == 'stilling'] #Stilling in dd_org!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'stilling' in distance_jaro.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Saving the best possible match (left=name found in the data, right=name found in the extended version of digdag)\n",
    "a = distance_jaro.idxmax().reset_index(name = 'potential_match')\n",
    "b = distance_jaro.max().reset_index(name='jaro')\n",
    "c = a.merge(b, on='index').rename(columns={'index':'data_name'})\n",
    "c.to_csv(_path+'../out/mapping.tsv', sep='\\t')\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced matching\n",
    "(This is the same as the basic matching but in this case it matches to the computed best match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select threshold to perform the matching, each mapping contains only the best jaro\n",
    "THRESHOLD = 0.9\n",
    "\n",
    "dfmap = pd.read_csv(_path+'../out/mapping.tsv', sep = '\\t', dtype=str, names=['original','mapped', 'score'], skiprows=1)\n",
    "dfmap = dfmap[dfmap.score.astype(float) >= THRESHOLD][['original','mapped']]\n",
    "dfmap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_path = '../../city_dump/data/utf8/'\n",
    "\n",
    "#This is where I save the counts and then group it together\n",
    "out_list = []\n",
    "\n",
    "\n",
    "for f in working_data:\n",
    "    \n",
    "    print(f)\n",
    "    #Loading the data\n",
    "    df = get_df(_path+f)\n",
    "    \n",
    "    #Dropping empty rows due to the \"mal-conversion\" to utf8... The empty rows do contain data originally\n",
    "    #TODO: Fix the conversion thing\n",
    "    df = df[~df.Herred.isna()]\n",
    "    print(len(df))\n",
    "    \n",
    "    #Preforming the name cleaning for Sogne, Herred, and Amt\n",
    "    for working_column in ['Sogne','Herred','Amt']:\n",
    "        df[working_column] = df.apply(lambda x: name_cleaner(x), axis=1)\n",
    "    \n",
    "    #Performing the matching for \"inexisting places\"\n",
    "    df = df.merge(dfmap, left_on='Sogne', right_on='original', how='left')\n",
    "    df.loc[~df.mapped.isna(), 'Sogne'] = df.loc[~df.mapped.isna(), 'mapped']\n",
    "\n",
    "\n",
    "    # Focusing on Sogn that match the Sogn list in DigDag\n",
    "    out = df.merge(dd[dd.art=='sogn'][['simplename', 'art','enhedid']], left_on='Sogne', right_on='simplename', how='left')\n",
    "    \n",
    "    #Counting the matches\n",
    "    out = out[~out.art.isna()].groupby('enhedid').size().reset_index(name='counts')\n",
    "    \n",
    "    # setting the year\n",
    "    out['year'] = f[2:6]\n",
    "    \n",
    "    #Keeping the counts on RAM\n",
    "    out_list.append(out)\n",
    "\n",
    "    \n",
    "#Concatenating all the files (the counts and missmatches)\n",
    "out_list = pd.concat(out_list, sort=False)\n",
    "\n",
    "\n",
    "#Saving the unique file in the HD\n",
    "out_list = out_list.merge(dd_org, on ='enhedid').pivot('simplename', 'year', 'counts').fillna(0).astype(int)\n",
    "out_list.to_csv(_path+'../out/counts_first_jaro.txt', sep='$')\n",
    "out_list.to_csv(_path+'../out/places_FT_jaro0.9_01.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "print('Done :D')\n",
    "\n",
    "nr_matched_records = out_list.sum().sum() #Two times sum, first one for year, and second one among years\n",
    "print('Matched records:',nr_matched_records, 'out of', total_records, '(', nr_matched_records/total_records*100,')%')\n",
    "print('Matched places:', len(out_list), 'out of', nr_sogn, '(', len(out_list)/nr_sogn*100,')%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding few more requested variables\n",
    "dfmap = pd.read_csv(_path+'../out/mapping.tsv', sep = '\\t', dtype=str, names=['original','mapped', 'score'], skiprows=1)\n",
    "jaro09 = pd.read_csv(_path+'../out/places_FT_jaro0.9_01.tsv', sep='\\t')\n",
    "jaro09 = jaro09.merge(dfmap[dfmap.score.astype(float) >= THRESHOLD], left_on='simplename', right_on='mapped').merge(dd[['simplename', 'art', 'enhedid']], on='simplename')\n",
    "jaro09.to_csv(_path+'../out/places_FT_jaro0.9_01.tsv', sep='\\t')\n",
    "jaro09.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the best possible matches\n",
    "\n",
    "concatenate = []\n",
    "i=0\n",
    "for col in distance_jaro.columns:\n",
    "    i = i+1\n",
    "    if i % 100 == 0: print(i, 'out of', len(distance_jaro.columns))\n",
    "    aux = distance_jaro.nlargest(5, col)[[col]]\n",
    "    aux.columns = ['score']\n",
    "    aux['original'] = col\n",
    "    concatenate.append(aux)\n",
    "\n",
    "concatenate = pd.concat(concatenate, sort=False)\n",
    "concatenate = concatenate.reset_index()\n",
    "concatenate.columns = ['potential_match','score','original']\n",
    "concatenate[['original','potential_match','score']][~concatenate.original.isin(concatenate[concatenate.score >= 0.9].original.unique())].to_csv(_path+'../out/places_possible_matches_jaro_01.tsv', index=False, sep='\\t')\n",
    "\n",
    "concatenate = pd.read_csv(_path+'../out/places_possible_matches_jaro_01.tsv',  sep='\\t')\n",
    "concatenate.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the counts for all possible matches (reading only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = '../../city_dump/data/utf8/'\n",
    "\n",
    "#This is where I save the counts and then group it together\n",
    "out_list = []\n",
    "\n",
    "\n",
    "for f in working_data:\n",
    "    \n",
    "    print(f)\n",
    "    #Loading the data\n",
    "    df = get_df(_path+f)\n",
    "    \n",
    "    #Dropping empty rows due to the \"mal-conversion\" to utf8... The empty rows do contain data originally\n",
    "    #TODO: Fix the conversion thing\n",
    "    df = df[~df.Herred.isna()]\n",
    "    print(len(df))\n",
    "    \n",
    "    #Preforming the name cleaning for Sogne, Herred, and Amt\n",
    "    for working_column in ['Sogne','Herred','Amt']:\n",
    "        df[working_column] = df.apply(lambda x: name_cleaner(x), axis=1)\n",
    "    \n",
    "    df['year'] = f[2:6]\n",
    "    \n",
    "    #TODO: Do the counting inside so it removes overhead\n",
    "    \n",
    "    #Keeping the counts on RAM\n",
    "    out_list.append(df)\n",
    "\n",
    "    \n",
    "#Concatenating all the files (the counts and missmatches)\n",
    "df = pd.concat(out_list, sort=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read everything and now do the counting outside (not the best... would be better to count inside the loop)\n",
    "dfc = df.groupby(['Sogne','year']).size().reset_index(name='counts')\n",
    "dfc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.9\n",
    "\n",
    "#One row per city\n",
    "a = dfc.rename(columns={'year':'id'}).pivot(index='Sogne', columns='id', values='counts').reset_index().fillna(0).astype(int, errors='ignore')\n",
    "\n",
    "#Performing the matching for mapped places\n",
    "dfmap = pd.read_csv(_path+'../out/mapping.tsv', sep = '\\t', dtype=str, names=['original','mapped', 'score'], skiprows=1)\n",
    "dfmap =  dfmap[dfmap.score.astype(float) >= THRESHOLD]\n",
    "\n",
    "df = a.merge(dfmap, left_on='Sogne', right_on='original', how='left')\n",
    "df.loc[~df.mapped.isna(), 'Sogne'] = df.loc[~df.mapped.isna(), 'mapped']\n",
    "\n",
    "# Focusing on Sogn that match the Sogn list in DigDag\n",
    "df = df.merge(dd[dd.art=='sogn'][['simplename', 'art','enhedid']], left_on='Sogne', right_on='simplename', how='left')\n",
    "df.loc[df.simplename.isna() & ~df.mapped.isna(), 'simplename'] = df.loc[df.simplename.isna() & ~df.mapped.isna(), 'mapped']\n",
    "df.loc[~df.simplename.isna(), 'Sogne'] = df.loc[~df.simplename.isna(), 'simplename']\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summing the mapped columns\n",
    "cols = df.columns\n",
    "df = df.merge(df.groupby(['Sogne'],as_index = False).agg({'1845':'sum', '1850':'sum','1860':'sum', '1880':'sum','1885':'sum', '1901':'sum'}), on='simplename')\n",
    "df.loc[~df['1845_y'].isna(), '1845'] = df.loc[~df['1845_y'].isna(), '1845_y']\n",
    "df.loc[~df['1845_y'].isna(), '1850'] = df.loc[~df['1845_y'].isna(), '1850_y']\n",
    "df.loc[~df['1845_y'].isna(), '1860'] = df.loc[~df['1845_y'].isna(), '1860_y']\n",
    "df.loc[~df['1845_y'].isna(), '1880'] = df.loc[~df['1845_y'].isna(), '1880_y']\n",
    "df.loc[~df['1845_y'].isna(), '1885'] = df.loc[~df['1845_y'].isna(), '1885_y']\n",
    "df.loc[~df['1845_y'].isna(), '1901'] = df.loc[~df['1845_y'].isna(), '1901_y']\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = concatenate.merge(df, left_on='original', right_on='Sogne').merge(df, left_on='potential_match', right_on='Sogne')\n",
    "for y in ['1845', '1850', '1860', '1880', '1885', '1901']:\n",
    "    z[y] = z[y+'_x'] + z[y+'_y']\n",
    "z = z[['original','potential_match', 'score', '1845', '1850', '1860', '1880', '1885', '1901', 'art', 'enhedid']].merge(dd.drop_duplicates('simplename', keep='first'), left_on='potential_match', right_on='simplename')\n",
    "\n",
    "z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z should have what I want"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
