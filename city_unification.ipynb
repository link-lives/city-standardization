{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries for data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Importing libraries for system management\n",
    "import os\n",
    "\n",
    "#Distance computation\n",
    "from pyjarowinkler import distance\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_want_to_run_all_the_preprocessing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the current format (ISO-8859-14) into UTF-8 \n",
    "iconv -f ISO-8859-14 file_in.csv > file_out.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if I_want_to_run_all_the_preprocessing:\n",
    "    \n",
    "    #Copying into a new directory\n",
    "    ! cp -R ../city_dump/data/LL/ ../city_dump/data/utf8/\n",
    "\n",
    "    #Getting all files\n",
    "    d_path = '../city_dump/data/utf8/'\n",
    "    l = next(os.walk(d_path))[2]\n",
    "\n",
    "    #Converting the files to utf8\n",
    "    for file in l:\n",
    "        f_path = d_path+file\n",
    "        tmp_path = d_path+'tmp.csv'\n",
    "        ! iconv -f ISO-8859-14 $f_path > $tmp_path\n",
    "        !mv $tmp_path $f_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading DigDag\n",
    "(and transforming it so I encode as many logical variation of the names as possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The digdag seems to have duplicates which I do not understand why\n",
    "dd = pd.read_csv('../city_dump/data/rl_places_digdag_v1.txt', sep = '\\t', encoding='utf-16', dtype=str)\n",
    "\n",
    "dd['simplename'] = dd['simplename'].astype(str)\n",
    "\n",
    "#adding a conversion of special characters to latin letters (å -> aa, å -> oe, æ -> ae) and adding them to the reference list\n",
    "dd['special'] = dd['simplename'].apply(lambda x: 'ø' in x) | dd['simplename'].apply(lambda x: 'å' in x) | dd['simplename'].apply(lambda x: 'æ' in x)\n",
    "dd['simplename2'] = dd.simplename.apply(lambda x: x.replace('å','aa').replace('ø','oe').replace('æ','ae'))\n",
    "dd = pd.concat([dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename']],dd[dd['special']][['navn', 'enhedid', 'enhedtype', 'art', 'simplename2']].rename(columns={'simplename2':'simplename'})])\n",
    "\n",
    "#Preparing the names to append them at the simple name\n",
    "dd['art'] =dd['art'].apply({'Amt':'amt', 'Sogn':'sogn','Købstadskommune':'købstad', 'Geografisk Herred':'herred', 'Processing':''}.get)\n",
    "\n",
    "#Adding a duplicated list of their unit type to increase matches\n",
    "dd['simplename2'] = dd.apply(lambda row: row['simplename']+' '+ row['art'], axis=1)\n",
    "dd = pd.concat([dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename']],dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename2']].rename(columns={'simplename2':'simplename'})])\n",
    "\n",
    "\n",
    "#Dropping duplicates from digdag\n",
    "#print(dd[dd.duplicated('simplename', keep=False)].sort_values('simplename').head(30))\n",
    "dd.drop_duplicates(['art','simplename'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the df from the $ separated file (for some reason pandas has problems with random lines)\n",
    "def get_df(f_path):\n",
    "    r= []\n",
    "    columns = []\n",
    "    with open(f_path) as f:\n",
    "        first = True\n",
    "        for line in f:\n",
    "            line = line.rstrip().split('$')\n",
    "            if first:\n",
    "                length = len(line)\n",
    "                columns = line\n",
    "                first=False\n",
    "            else:\n",
    "                r.append(line[:length])\n",
    "                \n",
    "    return pd.DataFrame(data=r, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name cleaning function\n",
    "def name_cleaner(s):\n",
    "    try:\n",
    "        o =s[working_column].lower().rstrip().replace('  ', ' ')#.replace(' købstad', '')\n",
    "        if ' (' in o: return o.split(' (')[0]\n",
    "        return o\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping all 1901 files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if I_want_to_run_all_the_preprocessing:\n",
    "    \n",
    "    l =['ft1901_LL_aalborg.txt',\n",
    "     'ft1901_LL_aarhus.txt',\n",
    "     'ft1901_LL_bornholm.txt',\n",
    "     'ft1901_LL_frederiksborg.txt',\n",
    "     'ft1901_LL_hjoerring.txt',\n",
    "     'ft1901_LL_holbaek.txt',\n",
    "     'ft1901_LL_kbhv.txt',\n",
    "     'ft1901_LL_maribo.txt',\n",
    "     'ft1901_LL_odense.txt',\n",
    "     'ft1901_LL_praestoe.txt',\n",
    "     'ft1901_LL_randers.txt',\n",
    "     'ft1901_LL_ribe.txt',\n",
    "     'ft1901_LL_ringkoebing.txt',\n",
    "     'ft1901_LL_roskilde.txt',\n",
    "     'ft1901_LL_skanderborg.txt',\n",
    "     'ft1901_LL_soroe.txt',\n",
    "     'ft1901_LL_svendborg.txt',\n",
    "     'ft1901_LL_thisted.txt',\n",
    "     'ft1901_LL_vejle.txt',\n",
    "     'ft1901_LL_viborg.txt']\n",
    "\n",
    "    _path = '../city_dump/data/utf8/'\n",
    "\n",
    "    df_list = []\n",
    "    for f in l:\n",
    "\n",
    "        print(f)\n",
    "        #Loading the data\n",
    "        df_list.append(get_df(_path+f))\n",
    "\n",
    "    #Concatenating all the files\n",
    "    df_list = pd.concat(df_list, sort=False)\n",
    "\n",
    "    #Saving the unique file\n",
    "    df_list.to_csv(_path+'ft1901_LL.txt', sep='$', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files to work (with the 1901 collapsed)\n",
    "working_data = ['ft1845_LL.txt',\n",
    " 'ft1850_LL.txt',\n",
    " 'ft1860_LL.txt',\n",
    " 'ft1880_LL.txt',\n",
    " 'ft1885_LL.txt',\n",
    " 'ft1901_LL.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic matching\n",
    "(only performs the match if the names are identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft1845_LL.txt\n",
      "1489875\n",
      "ft1850_LL.txt\n",
      "1391708\n",
      "ft1860_LL.txt\n",
      "1715906\n",
      "ft1880_LL.txt\n",
      "1967615\n",
      "ft1885_LL.txt\n",
      "327936\n",
      "ft1901_LL.txt\n",
      "1979025\n",
      "Done! :)\n"
     ]
    }
   ],
   "source": [
    "_path = '../city_dump/data/utf8/'\n",
    "\n",
    "#This is where I save the counts and then group it together\n",
    "out_list = []\n",
    "\n",
    "#where I save the missmatches and file year\n",
    "missmatches = []\n",
    "\n",
    "for f in working_data:\n",
    "    \n",
    "    print(f)\n",
    "    #Loading the data\n",
    "    df = get_df(_path+f)\n",
    "    \n",
    "    #Dropping empty rows due to the \"mal-conversion\" to utf8... The empty rows do contain data originally\n",
    "    #TODO: Fix the conversion thing\n",
    "    df = df[~df.Herred.isna()]\n",
    "    print(len(df))\n",
    "    \n",
    "    #Preforming the name cleaning for Sogne, Herred, and Amt\n",
    "    for working_column in ['Sogne','Herred','Amt']:\n",
    "        df[working_column] = df.apply(lambda x: name_cleaner(x), axis=1)\n",
    "\n",
    "    # Focusing on Sogn that match the Sogn list in DigDag\n",
    "    out = df.merge(dd[dd.art=='Sogn'][['simplename', 'art']], left_on='Sogne', right_on='simplename', how='left')\n",
    "\n",
    "    #keeping the missmatch\n",
    "    miss = out[out.art.isna()].drop_duplicates('Sogne', keep='first')\n",
    "    miss['year'] = f[2:6]\n",
    "    missmatches.append(miss)\n",
    "    \n",
    "    #Counting the matches\n",
    "    out = out[~out.art.isna()].groupby('simplename').size().reset_index(name='counts')\n",
    "    \n",
    "    # setting the year\n",
    "    out['year'] = f[2:6]\n",
    "    \n",
    "    #Keeping the counts on RAM\n",
    "    out_list.append(out)\n",
    "    \n",
    "#Concatenating all the files (the counts and missmatches)\n",
    "out_list = pd.concat(out_list, sort=False)\n",
    "missmatches = pd.concat(missmatches, sort=False)\n",
    "\n",
    "#Saving the unique file in the HD\n",
    "out_list = out_list.pivot('simplename', 'year', 'counts').fillna(0).astype(int)\n",
    "out_list.to_csv(_path+'../out/counts.txt', sep='$')\n",
    "out_list.to_csv(_path+'../out/counts.tsv', sep='\\t')\n",
    "print('Done! :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1901</td>\n",
       "      <td>2039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1880</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1845</td>\n",
       "      <td>1775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1860</td>\n",
       "      <td>1701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1850</td>\n",
       "      <td>1624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1885</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  counts\n",
       "5  1901    2039\n",
       "3  1880    2016\n",
       "0  1845    1775\n",
       "2  1860    1701\n",
       "1  1850    1624\n",
       "4  1885     558"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cities that could not be matched on the first round counts per year\n",
    "missmatches.groupby('year').size().reset_index(name='counts').sort_values('counts', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing city similarity for missmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 5800\n",
      "100 out of 5800\n",
      "200 out of 5800\n",
      "300 out of 5800\n",
      "400 out of 5800\n",
      "500 out of 5800\n",
      "600 out of 5800\n",
      "700 out of 5800\n",
      "800 out of 5800\n",
      "900 out of 5800\n",
      "1000 out of 5800\n",
      "1100 out of 5800\n",
      "1200 out of 5800\n",
      "1300 out of 5800\n",
      "1400 out of 5800\n",
      "1500 out of 5800\n",
      "1600 out of 5800\n",
      "1700 out of 5800\n",
      "1800 out of 5800\n",
      "1900 out of 5800\n",
      "2000 out of 5800\n",
      "2100 out of 5800\n",
      "2200 out of 5800\n",
      "2300 out of 5800\n",
      "2400 out of 5800\n",
      "2500 out of 5800\n",
      "2600 out of 5800\n",
      "2700 out of 5800\n",
      "2800 out of 5800\n",
      "2900 out of 5800\n",
      "3000 out of 5800\n",
      "3100 out of 5800\n",
      "3200 out of 5800\n",
      "3300 out of 5800\n",
      "3400 out of 5800\n",
      "3500 out of 5800\n",
      "3600 out of 5800\n",
      "3700 out of 5800\n",
      "3800 out of 5800\n",
      "3900 out of 5800\n",
      "4000 out of 5800\n",
      "4100 out of 5800\n",
      "4200 out of 5800\n",
      "4300 out of 5800\n",
      "4400 out of 5800\n",
      "4500 out of 5800\n",
      "4600 out of 5800\n",
      "4700 out of 5800\n",
      "4800 out of 5800\n",
      "4900 out of 5800\n",
      "5000 out of 5800\n",
      "5100 out of 5800\n",
      "5200 out of 5800\n",
      "5300 out of 5800\n",
      "5400 out of 5800\n",
      "5500 out of 5800\n",
      "5600 out of 5800\n",
      "5700 out of 5800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>harte</th>\n",
       "      <th>almind</th>\n",
       "      <th>viuf</th>\n",
       "      <th>seest</th>\n",
       "      <th>sønder vilstrup</th>\n",
       "      <th>eltang</th>\n",
       "      <th>nørre bjert</th>\n",
       "      <th>nørre bramdrup</th>\n",
       "      <th>gudum</th>\n",
       "      <th>tjæreby</th>\n",
       "      <th>...</th>\n",
       "      <th>esbjerg købstad</th>\n",
       "      <th>simmelkær</th>\n",
       "      <th>struer</th>\n",
       "      <th>herborg</th>\n",
       "      <th>grove</th>\n",
       "      <th>voel</th>\n",
       "      <th>silkeborg landsogn</th>\n",
       "      <th>silkeborg købstad</th>\n",
       "      <th>hov</th>\n",
       "      <th>engesvang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kronborg</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>præstø</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bornholms</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svendborg</th>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ålborg</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2819 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              harte    almind      viuf     seest  sønder vilstrup    eltang  \\\n",
       "kronborg   0.615385  0.571429  0.666667  0.615385         0.565217  0.500000   \n",
       "præstø     0.454545  0.500000  0.600000  0.363636         0.571429  0.500000   \n",
       "bornholms  0.571429  0.600000  0.692308  0.642857         0.541667  0.600000   \n",
       "svendborg  0.642857  0.533333  0.615385  0.500000         0.500000  0.466667   \n",
       "ålborg     0.545455  0.416667  0.600000  0.545455         0.619048  0.333333   \n",
       "\n",
       "           nørre bjert  nørre bramdrup     gudum   tjæreby    ...      \\\n",
       "kronborg      0.421053        0.500000  0.615385  0.533333    ...       \n",
       "præstø        0.588235        0.650000  0.545455  0.461538    ...       \n",
       "bornholms     0.500000        0.521739  0.571429  0.562500    ...       \n",
       "svendborg     0.450000        0.521739  0.571429  0.500000    ...       \n",
       "ålborg        0.529412        0.600000  0.545455  0.538462    ...       \n",
       "\n",
       "           esbjerg købstad  simmelkær    struer   herborg     grove      voel  \\\n",
       "kronborg          0.565217   0.529412  0.500000  0.266667  0.461538  0.583333   \n",
       "præstø            0.571429   0.600000  0.500000  0.461538  0.454545  0.600000   \n",
       "bornholms         0.500000   0.500000  0.533333  0.437500  0.500000  0.538462   \n",
       "svendborg         0.500000   0.444444  0.466667  0.250000  0.571429  0.538462   \n",
       "ålborg            0.571429   0.533333  0.500000  0.230769  0.454545  0.500000   \n",
       "\n",
       "           silkeborg landsogn  silkeborg købstad       hov  engesvang  \n",
       "kronborg             0.538462           0.520000  0.636364   0.470588  \n",
       "præstø               0.666667           0.608696  0.666667   0.533333  \n",
       "bornholms            0.518519           0.500000  0.583333   0.500000  \n",
       "svendborg            0.481481           0.461538  0.666667   0.444444  \n",
       "ålborg               0.541667           0.521739  0.555556   0.533333  \n",
       "\n",
       "[5 rows x 2819 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the potential first matches\n",
    "\n",
    "mm = missmatches.Sogne.unique()\n",
    "sn = dd.simplename.unique()\n",
    "\n",
    "distance_jaro = np.zeros((len(sn),len(mm)))\n",
    "distance_leven = np.zeros((len(sn),len(mm)))\n",
    "\n",
    "for i in range(len(sn)):\n",
    "    if i%100 == 0: print(i, 'out of' ,len(sn))\n",
    "        \n",
    "    for j in range(len(mm)):\n",
    "        #If variable is none skip\n",
    "        if not mm[j]: continue\n",
    "        \n",
    "        #This matrix is not simetric because x and y axis are not the same!\n",
    "        try:\n",
    "            distance_jaro[i][j] = distance.get_jaro_distance(sn[i],mm[j])\n",
    "            distance_leven[i][j] = Levenshtein.distance(sn[i],mm[j])/(len(sn[i])+len(mm[j]))\n",
    "        except:\n",
    "            print('Could not make it',i,j)\n",
    "            print(sn[i],mm[j])\n",
    "        \n",
    "distance_jaro = pd.DataFrame(data = distance_jaro, columns = mm, index = sn)\n",
    "distance_leven = pd.DataFrame(data = distance_leven, columns = mm, index = sn)\n",
    "distance_leven.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving a mapping\n",
    "distance_jaro.idxmax().to_csv(_path+'../out/mapping.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced matching\n",
    "(This is the same as the basic matching but in this case it matches to the computed best match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harte</td>\n",
       "      <td>harte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>almind</td>\n",
       "      <td>almind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>viuf</td>\n",
       "      <td>viuf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seest</td>\n",
       "      <td>seest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sønder vilstrup</td>\n",
       "      <td>sønder vilstrup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          original           mapped\n",
       "0            harte            harte\n",
       "1           almind           almind\n",
       "2             viuf             viuf\n",
       "3            seest            seest\n",
       "4  sønder vilstrup  sønder vilstrup"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmap = pd.read_csv(_path+'../out/mapping.tsv', sep = '\\t', dtype=str, names=['original','mapped'])\n",
    "dfmap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft1845_LL.txt\n",
      "1489875\n"
     ]
    }
   ],
   "source": [
    "_path = '../city_dump/data/utf8/'\n",
    "\n",
    "#This is where I save the counts and then group it together\n",
    "out_list = []\n",
    "\n",
    "\n",
    "for f in working_data:\n",
    "    \n",
    "    print(f)\n",
    "    #Loading the data\n",
    "    df = get_df(_path+f)\n",
    "    \n",
    "    #Dropping empty rows due to the \"mal-conversion\" to utf8... The empty rows do contain data originally\n",
    "    #TODO: Fix the conversion thing\n",
    "    df = df[~df.Herred.isna()]\n",
    "    print(len(df))\n",
    "    \n",
    "    #Preforming the name cleaning for Sogne, Herred, and Amt\n",
    "    for working_column in ['Sogne','Herred','Amt']:\n",
    "        df[working_column] = df.apply(lambda x: name_cleaner(x), axis=1)\n",
    "    \n",
    "    #Performing the matching for \"inexisting places\"\n",
    "    df = df.merge(dfmap, left_on='Sogne', right_on='original', how='left')\n",
    "    df.loc[~df.mapped.isna(), 'Sogne'] = df.loc[~df.mapped.isna(), 'mapped']\n",
    "\n",
    "\n",
    "    # Focusing on Sogn that match the Sogn list in DigDag\n",
    "    out = df.merge(dd[dd.art=='Sogn'][['simplename', 'art']], left_on='Sogne', right_on='simplename', how='left')\n",
    "    \n",
    "    #Counting the matches\n",
    "    out = out[~out.art.isna()].groupby('simplename').size().reset_index(name='counts')\n",
    "    \n",
    "    # setting the year\n",
    "    out['year'] = f[2:6]\n",
    "    \n",
    "    #Keeping the counts on RAM\n",
    "    out_list.append(out)\n",
    "    \n",
    "#Concatenating all the files (the counts and missmatches)\n",
    "out_list = pd.concat(out_list, sort=False)\n",
    "\n",
    "#Saving the unique file in the HD\n",
    "out_list = out_list.pivot('simplename', 'year', 'counts').fillna(0).astype(int)\n",
    "out_list.to_csv(_path+'../out/counts_first_jaro.txt', sep='$')\n",
    "out_list.to_csv(_path+'../out/counts_first_jaro.tsv', sep='\\t')\n",
    "print('Done! :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list so they can evaluate the matching correctness\n",
    "\n",
    "concatenate = []\n",
    "i=0\n",
    "for col in distance_jaro.columns:\n",
    "    i = i+1\n",
    "    if i % 100: print(i)\n",
    "    aux = distance_jaro.nlargest(5, col)[[col]]\n",
    "    aux.columns = ['score']\n",
    "    aux['original'] = col\n",
    "    concatenate.append(aux)\n",
    "\n",
    "concatenate = pd.concat(concatenate, sort=False)\n",
    "concatenate = concatenate.reset_index()\n",
    "concatenate.columns = ['potential_match','score','original']\n",
    "concatenate[['original','potential_match','score']].to_csv(_path+'../out/possible_matches.tsv', sep='\\t')\n",
    "concatenate.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Perfect match:', len(concatenate[concatenate.score == 1].original.unique()))\n",
    "print('To match:', len(concatenate[~concatenate.original.isin(concatenate[concatenate.score == 1].original.unique())].sort_values('score', ascending=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate[~concatenate.original.isin(concatenate[concatenate.score == 1].original.unique())].sort_values('score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
