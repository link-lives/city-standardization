{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birthplace (fødested) cleaning\n",
    "Here I inted to clean the birthplace (fødested) field in the data. Notice that the pre-processing scripts are in city_unification (conversion to UTF8, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries for data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Importing libraries for system management\n",
    "import os\n",
    "\n",
    "#Distance computation\n",
    "from pyjarowinkler import distance\n",
    "\n",
    "#Regular expressions\n",
    "import re\n",
    "\n",
    "#Importing multiprocessing libraries\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Worning directories\n",
    "in_folder  = '/data/import/nirama/utf8/'\n",
    "out_folder = '/home/roregu/workspace/out/'\n",
    "data_folder = '/home/roregu/workspace/data/'\n",
    "\n",
    "#Column to process\n",
    "focus_column = 'fødested'\n",
    "\n",
    "#The Jaro threshold\n",
    "THRESHOLD = 0.9\n",
    "\n",
    "#Number of cores (machine = 28)\n",
    "n_cores = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading DigDag\n",
    "and expanding with variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function adds extra rows in the dataframe *dd* for the \n",
    "# places where *word* exists and replaces it with *replacement*\n",
    "def adding_extra_rows(dd, word, replacement):\n",
    "    dd['special'] = dd['simplename'].apply(lambda x: word in x) \n",
    "    dd['simplename2'] = dd.simplename.apply(lambda x: x.replace(word,replacement))\n",
    "    return pd.concat([dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename']],dd[dd['special']][['navn', 'enhedid', 'enhedtype', 'art', 'simplename2']].rename(columns={'simplename2':'simplename'})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>navn</th>\n",
       "      <th>enhedid</th>\n",
       "      <th>enhedtype</th>\n",
       "      <th>art</th>\n",
       "      <th>simplename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Kronborg Amt</td>\n",
       "      <td>118765</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>kronborg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Præstø Amt</td>\n",
       "      <td>118791</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>præstø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Bornholms Amt</td>\n",
       "      <td>118792</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>bornholms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Svendborg Amt</td>\n",
       "      <td>118813</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>svendborg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Ålborg Amt</td>\n",
       "      <td>118819</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>ålborg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Sorø Amt</td>\n",
       "      <td>118785</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>sorø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Århus Amt</td>\n",
       "      <td>118846</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>århus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Hjørring Amt</td>\n",
       "      <td>118820</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>hjørring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Bøvling Amt</td>\n",
       "      <td>118851</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>bøvling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Vejle Amt</td>\n",
       "      <td>118849</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>vejle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Dueholm Amt</td>\n",
       "      <td>118821</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>dueholm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Ørum Amt</td>\n",
       "      <td>118824</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>ørum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Thisted Amt</td>\n",
       "      <td>118826</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>thisted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Viborg Amt</td>\n",
       "      <td>118830</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>viborg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Ringkøbing Amt</td>\n",
       "      <td>118853</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>ringkøbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Københavns Kommune</td>\n",
       "      <td>370923</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>københavns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Frederiksberg Kommune</td>\n",
       "      <td>370924</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>frederiksberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Fyns Amt</td>\n",
       "      <td>118739</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>fyns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Sæbygård Amt</td>\n",
       "      <td>118773</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>sæbygård</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Ribe Amt</td>\n",
       "      <td>118855</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>ribe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     navn enhedid enhedtype  art     simplename\n",
       "0            Kronborg Amt  118765        11  amt       kronborg\n",
       "1              Præstø Amt  118791        11  amt         præstø\n",
       "2           Bornholms Amt  118792        11  amt      bornholms\n",
       "3           Svendborg Amt  118813        11  amt      svendborg\n",
       "4              Ålborg Amt  118819        11  amt         ålborg\n",
       "5                Sorø Amt  118785        11  amt           sorø\n",
       "6               Århus Amt  118846        11  amt          århus\n",
       "7            Hjørring Amt  118820        11  amt       hjørring\n",
       "8             Bøvling Amt  118851        11  amt        bøvling\n",
       "9               Vejle Amt  118849        11  amt          vejle\n",
       "10            Dueholm Amt  118821        11  amt        dueholm\n",
       "11               Ørum Amt  118824        11  amt           ørum\n",
       "12            Thisted Amt  118826        11  amt        thisted\n",
       "13             Viborg Amt  118830        11  amt         viborg\n",
       "14         Ringkøbing Amt  118853        11  amt     ringkøbing\n",
       "15     Københavns Kommune  370923        11  amt     københavns\n",
       "16  Frederiksberg Kommune  370924        11  amt  frederiksberg\n",
       "17               Fyns Amt  118739        11  amt           fyns\n",
       "18           Sæbygård Amt  118773        11  amt       sæbygård\n",
       "19               Ribe Amt  118855        11  amt           ribe"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The digdag seems to have duplicates which I do not understand why\n",
    "dd = pd.read_csv(data_folder+'rl_places_digdag_v1.txt', sep = '\\t', encoding='utf-16', dtype=str)\n",
    "dd_org = dd.copy()\n",
    "\n",
    "#Getting the Købstad and putting it to the original data\n",
    "dd2 = pd.read_csv(data_folder+'koebstad.csv', sep = ';', encoding='utf-8', dtype=str)\n",
    "dd2['enhedtype'] = '80'#dd2['art'] \n",
    "dd = pd.concat([dd, dd2[~dd2.isna()][['navn','enhedid','enhedtype','art','simplename']]])\n",
    "dd['simplename'] = dd['simplename'].astype(str)\n",
    "del dd2\n",
    "\n",
    "#Moving the Købstad to the right column\n",
    "dd['art'] = np.where(dd['enhedtype'] == 'Købstad', ['Købstadskommune']*len(dd), dd['art'])\n",
    "\n",
    "#adding a conversion of special characters to latin letters (å -> aa, ø -> oe, æ -> ae) and adding them to the reference list\n",
    "dd = adding_extra_rows(dd, 'å', 'aa')\n",
    "dd = adding_extra_rows(dd, 'ø', 'oe')\n",
    "dd = adding_extra_rows(dd, 'æ', 'ae')\n",
    "\n",
    "#Preparing the names to append them at the simple name\n",
    "dd['art'] = dd['art'].apply({'Amt':'amt', 'Sogn':'sogn','Købstadskommune':'købstad', 'Geografisk Herred':'herred', 'Processing':''}.get)\n",
    "\n",
    "#Adding a duplicated list of their unit type to increase matches\n",
    "dd['simplename2'] = dd.apply(lambda row: str(row['simplename'])+' '+str(row['art']), axis=1)\n",
    "dd = pd.concat([dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename']],dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename2']].rename(columns={'simplename2':'simplename'})])\n",
    "\n",
    "#Adding the plural købstæder as well\n",
    "dd = adding_extra_rows(dd, 'købstad', 'købstæder')\n",
    "\n",
    "#Adding spaces instead of hyphens (new rows)\n",
    "dd = adding_extra_rows(dd, '-', ' ')\n",
    "\n",
    "#Removing spaces (new rows)\n",
    "dd = adding_extra_rows(dd, ' ', '')\n",
    "\n",
    "#Adding bysogn and landsogn (new rows)\n",
    "dd = adding_extra_rows(dd, 'sogn', 'bysogn')\n",
    "dd = adding_extra_rows(dd, 'sogn', 'landsogn')\n",
    "\n",
    "#adding rows where the last letter's \"e\" will be removed\n",
    "dd['special'] = dd['simplename'].str[-1] == 'e'\n",
    "dd['simplename2'] = dd['simplename'].str[:-1]\n",
    "dd = pd.concat([dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename']],dd[dd['special']][['navn', 'enhedid', 'enhedtype', 'art', 'simplename2']].rename(columns={'simplename2':'simplename'})])\n",
    "\n",
    "#Dropping duplicates from digdag\n",
    "#print(dd[dd.duplicated('simplename', keep=False)].sort_values('simplename').head(30))\n",
    "dd.drop_duplicates(['art','simplename'], keep='first', inplace=True)\n",
    "\n",
    "#Drop 'Non' from the list, this may be one of my artifacts...\n",
    "dd = dd[(dd.simplename.apply(lambda x: not 'Non' in x))]\n",
    "\n",
    "#Adding sweeden into the equation\n",
    "dd = dd.append({'navn':'Sweden','enhedid':400000,'enhedtype':'90','art':'country','simplename':'sverige'}, ignore_index=True).append({'navn':'Sweden','enhedid':400000,'enhedtype':'90','art':'country','simplename':'sverrig'}, ignore_index=True)\n",
    "\n",
    "# TODO take into consideration that the same city name can be in varios geographical areas...\n",
    "# It should be matched with herred to increase accuracy for each record. However, right now we're only cleaning\n",
    "# not sure how pressing the issue is...\n",
    "nr_sogn = len(dd_org[dd_org.art=='Sogn'].simplename.unique()) \n",
    "dd.head(20)\n",
    "\n",
    "#TODO: Right now I can map things to amt for example which should not be the case...\n",
    "#dd =dd[dd.art='sogn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding countries and islands\n",
    "Countries: Barbara's list, but missing countries before the german, and italian unifications\n",
    "\n",
    "Islands: From mads from Danmark's Stednavne (You'll find the standardized name in the \"opslagsform\" column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length original:\t 2519 \n",
      "length modified:\t 26874 \n",
      "unique sogn keys:\t 1924\n"
     ]
    }
   ],
   "source": [
    "print('length original:\\t', len(dd_org), '\\nlength modified:\\t', len(dd), '\\nunique sogn keys:\\t', nr_sogn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the df from the $ separated file (for some reason pandas has problems with random lines)\n",
    "def get_df(f_path):\n",
    "    r= []\n",
    "    columns = []\n",
    "    with open(f_path) as f:\n",
    "        first = True\n",
    "        for line in f:\n",
    "            line = line.rstrip().split('$')\n",
    "            if first:\n",
    "                length = len(line)\n",
    "                columns = line\n",
    "                first=False\n",
    "            else:\n",
    "                r.append(line[:length])\n",
    "                \n",
    "    return pd.DataFrame(data=r, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name cleaning function\n",
    "def name_cleaner(s, working_column):\n",
    "    try:\n",
    "        o = s[working_column].lower().strip().replace('  ', ' ').replace('*', '').replace('?', '').replace('.', '')\n",
    "        \n",
    "        #If \"do\" (ditto= same as above) and people write what is same, then I keep what they wrote\n",
    "        if o[:3] == 'do ':\n",
    "            o = re.findall(r'do\\.?\\s\\[?\\(?(.+)',o)[0]\n",
    "            \n",
    "        #Get some clarifications\n",
    "        if o[:4] == 'her ':\n",
    "            o = re.findall(r'her\\.?\\s\\[?\\(?(.+)',o)[0]\n",
    "            \n",
    "        #if it says \"her i sogn\", I get the value for \"Sogn\" and put it there\n",
    "        if ('heri sogn' in o or 'her i sogn' in o or 'her i s' == o or 'i sognet' == o[:8] or 'født i sognet' == o or 'sognet' == o or 'her' == o or 'her i byen'  == o or 'h i s'  == o or 'heri s'  == o or 'h i sognet' == o) and working_column != 'Sogne':\n",
    "            return name_cleaner(s, 'Sogne')\n",
    "        \n",
    "        # Removing variability not catched by Jaro for Copenhagen\n",
    "        if o in 'kbhvn*kbhv*kjøbenh*kjøbh*kbhvn*kbhavn*kjøbhvn':\n",
    "            return 'kobenhavn'\n",
    "\n",
    "        return o\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files to work (with the 1901 collapsed)\n",
    "#This I did\n",
    "working_data = ['ft1845_LL.txt',\n",
    " 'ft1850_LL.txt',\n",
    " 'ft1860_LL.txt',\n",
    " 'ft1880_LL.txt',\n",
    " 'ft1885_LL.txt',\n",
    " 'ft1901_LL.txt']\n",
    "\n",
    "#This Nicolai\n",
    "working_data = ['ft1787.txt',\n",
    "'ft1801.txt',\n",
    "'ft1803.txt',\n",
    "'ft1834.txt',\n",
    "'ft1840.txt',\n",
    "'ft1845.txt',\n",
    "'ft1850.txt',\n",
    "'ft1860.txt',\n",
    "'ft1880.txt',\n",
    "'ft1885.txt',\n",
    "'ft1901.txt']\n",
    "\n",
    "#This Nicolai\n",
    "working_data = ['ft1845.txt',\n",
    "'ft1850.txt',\n",
    "'ft1860.txt',\n",
    "'ft1880.txt',\n",
    "'ft1885.txt',\n",
    "'ft1901.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft1845.txt\n",
      "ft1850.txt\n",
      "ft1901.txt\n",
      "ft1860.txt\n",
      "ft1880.txt\n",
      "ft1885.txt\n",
      "318056\n",
      "1359777\n",
      "1281746\n",
      "1700886\n",
      "1759664\n",
      "2046684\n",
      "done ft1901.txt\n",
      "done ft1901.txt\n",
      "done ft1901.txt\n",
      "done ft1901.txt\n",
      "done ft1901.txt\n",
      "done ft1901.txt\n",
      "Done! :D\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-ef7332b44105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mout_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done! :D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mtotal_people\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtotal_num_places\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfocus_column\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#This is where I save the counts and then group it together\n",
    "out_list = []\n",
    "\n",
    "#where I save the missmatches and file year\n",
    "missmatches = []\n",
    "total_records = 0\n",
    "\n",
    "#for f in working_data:\n",
    " \n",
    "def data_loader(f):\n",
    "    \n",
    "    print(f)\n",
    "    #Loading the data\n",
    "    df = get_df(in_folder+f)\n",
    "    \n",
    "    #Dropping empty rows due to the \"mal-conversion\" to utf8... The empty rows do contain data originally\n",
    "    df = df[~df[focus_column].isna() & (df[focus_column] !='')]\n",
    "    print(len(df))\n",
    "    \n",
    "    #Preforming the name cleaning, be aware that I replace \"her i sogn\" in there\n",
    "    df[focus_column+'_data'] = df.apply(lambda row: name_cleaner(row, focus_column), axis=1)\n",
    "    \n",
    "    #Reducing Dimensionality and saving in RAM\n",
    "    out = df.groupby(focus_column+'_data').size().reset_index(name = 'counts')\n",
    "    out['year'] = f[2:6]\n",
    "    return out\n",
    "\n",
    "with multiprocessing.Pool(processes=n_cores) as pool:\n",
    "    for result in pool.starmap(data_loader, zip(working_data)):\n",
    "        print('done', f)\n",
    "        out_list.append(result)\n",
    "\n",
    "out_list = pd.concat(out_list, sort=False)\n",
    "out_list.sort_values('counts',ascending=False)\n",
    "print('Done! :D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Putting it nicely, on a more horitzontal table\n",
    "out_list = out_list.pivot(focus_column+'_data', 'year', 'counts').fillna(0).astype(int).reset_index().reset_index(drop=True)\n",
    "out_list = pd.DataFrame(out_list.values, columns=out_list.columns.tolist()) #Getting rid of the wrong index\n",
    "out_list['total'] = out_list.apply(lambda row: row['1845']+row['1850']+row['1860']+row['1880']+row['1885']+row['1901'],axis=1)\n",
    "out_list.sort_values('total', ascending=False, inplace=True)\n",
    "out_list.total.sum() #Around the 14k first places compule 80% of the people\n",
    "out_list.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name splitting \n",
    "if there is sogn and amt in the same value I keep the amt and save the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_splitting(row):\n",
    "    r = row[focus_column+'_data'].split(',')\n",
    "    \n",
    "    try:\n",
    "        s = r[0]\n",
    "        a = r[1]\n",
    "    except:\n",
    "        s = r[0]\n",
    "        a = r[0]\n",
    "    \n",
    "    for i in range(len(r)):\n",
    "        if 'sogn' in r[i] or 'købstad' in r[i]: \n",
    "            if 'sogn' == r[i].strip(' ') or 'købstad' == r[i].strip(' ') : s = r[i-1]\n",
    "            else: s = r[i]\n",
    "        elif 'amt' in r[i]: \n",
    "            if 'amt' == r[i].strip(' '): a = r[i-1]\n",
    "            else: a = r[i]\n",
    "                \n",
    "    #When the comma does not separate\n",
    "    if s == a and (a[-4:] != 'sogn' or a[-7:] != 'købstad'):\n",
    "        if 'sogn' in a:\n",
    "            r = a.split('sogn')\n",
    "            return r[0], r[1]\n",
    "        elif 'købstad' in a:\n",
    "            r = a.split('købstad')\n",
    "            return r[0], r[1]\n",
    "    \n",
    "    return s, a\n",
    "\n",
    "\n",
    "#commas[[focus_column+'_sogn_data', focus_column+'_amt_data']] = commas[focus_column+'_data'].str.split(\",\", n=1,expand=True) \n",
    "out_list[focus_column+'_org'] = out_list[focus_column+'_data']\n",
    "out_list[[focus_column+'_data', focus_column+'_amt_data']] = out_list.apply(do_splitting, axis=1, result_type=\"expand\")\n",
    "out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo: Check wich sogn are repeated within different amts and treat them differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice that I only match for Sogn\n",
    "out = out_list.merge(dd[['navn','simplename', 'art', 'enhedid']], left_on=focus_column+'_data', right_on='simplename', how='left')\n",
    "\n",
    "#Those are the missmatch\n",
    "miss = out[out.enhedid.isna()]\n",
    "miss.head()\n",
    "\n",
    "#Here are the match\n",
    "match = out[~out.enhedid.isna()]\n",
    "match.head()\n",
    "\n",
    "# Aggregating the match by enhedid and year\n",
    "agg_counts = match.groupby(['navn', 'enhedid', 'art'],as_index = False).agg({'1845':'sum', '1850':'sum','1860':'sum', '1880':'sum','1885':'sum', '1901':'sum'})\n",
    "\n",
    "# Grouping all enhedid to see how many different versions of the same Sogne we have in the data\n",
    "diff_counts = match.groupby(['navn','enhedid'])[focus_column+'_data'].apply(set).reset_index(name='extended_digdag') #if parenthesis is confusing add **.apply(lambda x: ', '.join(x))** before the reset index\n",
    "\n",
    "# Joining DFs and saving\n",
    "s = agg_counts.merge(diff_counts, on = ['navn','enhedid'])\n",
    "s.to_csv(out_folder+'birthplaces_FT_uniquevalues_01.tsv', sep='\\t', index=False)\n",
    "print(len(s))\n",
    "s.head()\n",
    "del s, agg_counts, diff_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Jaro distances for the missmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaro(a, bb, i):\n",
    "    o=[]\n",
    "    for b in bb:\n",
    "        try:\n",
    "            o.append(distance.get_jaro_distance(a, b))\n",
    "        except:\n",
    "            o.append(0)\n",
    "    return i, o\n",
    "\n",
    "\n",
    "mm = miss[focus_column+'_data'].unique()\n",
    "sn = dd_org.simplename.unique() # Using the short version of dig dag to make it much faster (>x10)\n",
    "distance_jaro = np.zeros((len(sn),len(mm)))\n",
    "\n",
    "\n",
    "with multiprocessing.Pool(processes=n_cores) as pool:\n",
    "    for i , result in pool.starmap(compute_jaro, zip(sn, [mm]*len(sn), range(len(sn)))):\n",
    "        if i%100 == 0: print(i, 'out of' , len(sn))\n",
    "        distance_jaro[i,:] = result\n",
    "\n",
    "        \n",
    "#Index is the reference names found in digdag the columns the original data\n",
    "distance_jaro = pd.DataFrame(data = distance_jaro, columns = mm, index = sn)\n",
    "print(distance_jaro.shape, 'len mm,sn:', len(mm), len(sn)) #(26428, 1237)\n",
    "del mm, sn\n",
    "distance_jaro.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the best match with a minimum threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the highest match for each Sogne and filtering threshold\n",
    "a = distance_jaro.idxmax().reset_index(name = 'potential_match')\n",
    "b = distance_jaro.max().reset_index(name='jaro')\n",
    "dfmap = a.merge(b, on='index').rename(columns={'index':'data_name'})\n",
    "dfmap.to_csv(out_folder+'mapping_birthplaces.tsv', sep='\\t', index=False)\n",
    "dfmap = dfmap[dfmap.jaro.astype(float) >= THRESHOLD]\n",
    "dfmap.head()\n",
    "del a\n",
    "del b\n",
    "\n",
    "#Getting the mapped info into DigDag v2 (not really the v2)\n",
    "dd2 = dfmap.merge(dd[['navn','simplename', 'art', 'enhedid']].sort_values('art', ascending=False).drop_duplicates('simplename', keep='first'), left_on='potential_match', right_on='simplename')\n",
    "dd2['simplename'] = dd2['data_name']\n",
    "dd2 = dd2[['navn','enhedid','art','simplename']]\n",
    "dd2 = pd.concat([dd2[['navn','simplename', 'art', 'enhedid']], dd], sort=False) #putting together\n",
    "dd2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting again with the merges\n",
    "out = out_list.merge(dd2, left_on=focus_column+'_data', right_on='simplename', how='left')\n",
    "\n",
    "#Those are the missmatch\n",
    "miss = out[out.enhedid.isna()]\n",
    "miss.head()\n",
    "\n",
    "#Here are the match\n",
    "match = out[~out.enhedid.isna()]\n",
    "match.head()\n",
    "\n",
    "# Aggregating the match by enhedid and year\n",
    "agg_counts = match.groupby(['navn', 'enhedid', 'art'],as_index = False).agg({'1845':'sum', '1850':'sum','1860':'sum', '1880':'sum','1885':'sum', '1901':'sum'})\n",
    "\n",
    "# Grouping all enhedid to see how many different versions of the same Sogne we have in the data\n",
    "diff_counts = match.groupby(['navn','enhedid'])[focus_column+'_data'].apply(set).reset_index(name='extended_digdag') #if parenthesis is confusing add **.apply(lambda x: ', '.join(x))** before the reset index\n",
    "\n",
    "# Joining DFs and saving\n",
    "s = agg_counts.merge(diff_counts, on = ['navn','enhedid'])\n",
    "s.to_csv(out_folder+'birthplaces_FT_jaro0.9_01.tsv', sep='\\t', index=False)\n",
    "print(len(s))\n",
    "s.head()\n",
    "del s, agg_counts, diff_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the missmatches after jaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some of the missmatches have sogn in the name, so I will do another jaro for \"XXXX sogn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaro(a, bb, i):\n",
    "    o=[]\n",
    "    for b in bb:\n",
    "        try:\n",
    "            o.append(distance.get_jaro_distance(a, b))\n",
    "        except:\n",
    "            o.append(0)\n",
    "    return i, o\n",
    "\n",
    "\n",
    "mm = miss[focus_column+'_data'].unique()\n",
    "sn = dd[dd.simplename.str[-5:] == ' sogn'].simplename.unique() # Using the short version of dig dag to make it much faster (>x10)\n",
    "distance_jaro = np.zeros((len(sn),len(mm)))\n",
    "\n",
    "\n",
    "with multiprocessing.Pool(processes=n_cores) as pool:\n",
    "    for i , result in pool.starmap(compute_jaro, zip(sn, [mm]*len(sn), range(len(sn)))):\n",
    "        if i%100 == 0: print(i, 'out of' , len(sn))\n",
    "        distance_jaro[i,:] = result\n",
    "\n",
    "        \n",
    "#Index is the reference names found in digdag the columns the original data\n",
    "distance_jaro = pd.DataFrame(data = distance_jaro, columns = mm, index = sn)\n",
    "print(distance_jaro.shape, 'len mm,sn:', len(mm), len(sn)) #(26428, 1237)\n",
    "del mm, sn\n",
    "distance_jaro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the highest match for each Sogne and filtering threshold\n",
    "a = distance_jaro.idxmax().reset_index(name = 'potential_match')\n",
    "b = distance_jaro.max().reset_index(name='jaro')\n",
    "dfmap = a.merge(b, on='index').rename(columns={'index':'data_name'})\n",
    "dfmap.to_csv(out_folder+'mapping_birthplaces.tsv', sep='\\t', index=False)\n",
    "dfmap = dfmap[dfmap.jaro.astype(float) >= THRESHOLD]\n",
    "dfmap.head()\n",
    "del a\n",
    "del b\n",
    "\n",
    "#Getting the mapped info into DigDag v2 (not really the v2)\n",
    "dd3 = dfmap.merge(dd2[['navn','simplename', 'art', 'enhedid']].sort_values('art', ascending=False).drop_duplicates('simplename', keep='first'), left_on='potential_match', right_on='simplename')\n",
    "dd3['simplename'] = dd3['data_name']\n",
    "dd3 = dd3[['navn','enhedid','art','simplename']]\n",
    "dd3 = pd.concat([dd2[['navn','simplename', 'art', 'enhedid']], dd], sort=False) #putting together\n",
    "dd3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting again with the merges\n",
    "out = out_list.merge(dd3, left_on=focus_column+'_data', right_on='simplename', how='left')\n",
    "\n",
    "#Those are the missmatch\n",
    "miss = out[out.enhedid.isna()]\n",
    "miss.head()\n",
    "\n",
    "#Here are the match\n",
    "match = out[~out.enhedid.isna()]\n",
    "match.head()\n",
    "\n",
    "# Aggregating the match by enhedid and year\n",
    "agg_counts = match.groupby(['navn', 'enhedid', 'art'],as_index = False).agg({'1845':'sum', '1850':'sum','1860':'sum', '1880':'sum','1885':'sum', '1901':'sum'})\n",
    "\n",
    "# Grouping all enhedid to see how many different versions of the same Sogne we have in the data\n",
    "diff_counts = match.groupby(['navn','enhedid'])[focus_column+'_data'].apply(set).reset_index(name='extended_digdag') #if parenthesis is confusing add **.apply(lambda x: ', '.join(x))** before the reset index\n",
    "\n",
    "# Joining DFs and saving\n",
    "s = agg_counts.merge(diff_counts, on = ['navn','enhedid'])\n",
    "s.to_csv(out_folder+'birthplaces_FT_jaro0.9_02.tsv', sep='\\t', index=False)\n",
    "print(len(s))\n",
    "s.head()\n",
    "del s, agg_counts, diff_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_org[dd_org.simplename.str[:5] == 'køben']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.get_jaro_distance('kjøbenhavn', 'københavn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(miss.shape)\n",
    "miss.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "0/0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding DigDag info and saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the digDag info for the matches but also for the non matches\n",
    "\n",
    "m = concatenate.merge(dd2[['navn','simplename', 'art', 'enhedid']].sort_values('art', ascending=False).drop_duplicates('simplename',keep='first'), left_on='potential_match', right_on='simplename', how='inner')\n",
    "m = m[['original','potential_match','score','navn','art','enhedid', 'simplename']]\n",
    "\n",
    "w = dd2.sort_values('art', ascending=False).drop_duplicates('simplename',keep='first')\n",
    "w['potential_match'] = w['simplename']\n",
    "w['original'] = w['simplename']\n",
    "w['score'] = 1\n",
    "\n",
    "m = pd.concat([w, m], sort=True)\n",
    "\n",
    "\n",
    "#Putting the things into place\n",
    "m['original'] = np.where(m.original.isna(), m['simplename'], m['original'])\n",
    "\n",
    "print(len(concatenate), len(m))\n",
    "m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the counts for the original plus the counts for the potential matches in DigDag\n",
    "#Warning: Potential matches may not exist in the data...\n",
    "out = m.merge(out_list, left_on='original', right_on=focus_column+'_data', how='left').merge(out_list, left_on='potential_match', right_on=focus_column+'_data', how='left').fillna(0)\n",
    "for y in ['1845', '1850', '1860', '1880', '1885', '1901']:\n",
    "    out[y] = out[y+'_x'] + out[y+'_y']\n",
    "out = out[['original', 'potential_match', 'score', 'navn', 'art', 'enhedid', '1845', '1850', '1860', '1880', '1885', '1901']].sort_values('original')\n",
    "out.to_csv(_path+'../out/birthplaces_possible_matches_jaro_01.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done! :D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating file to check the validity of the mapping jaro 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(out_folder+'birthplaces_FT_jaro0.9_01.tsv', sep = '\\t', dtype=str)\n",
    "\n",
    "max_example = 5\n",
    "\n",
    "names    = df.navn.tolist()\n",
    "extendeds = df.extended_digdag.tolist()\n",
    "\n",
    "out = []\n",
    "for name, extended in zip(names, extendeds):\n",
    "    extended = list(eval(extended))\n",
    "    #m = min(len(extended), max_example)\n",
    "    samp = random.sample(extended, k= min(len(extended), max_example))\n",
    "    for s in samp:\n",
    "        out.append([name, s])\n",
    "\n",
    "dfout = pd.DataFrame(out, columns= ['DigDag', 'matches (0.9)'])\n",
    "dfout.to_csv(out_folder+'validity_jaro.tsv', sep='\\t', index=False)\n",
    "\n",
    "! cp $out_folder/validity_jaro.tsv /data/import/roc/validity_jaro.tsv\n",
    "\n",
    "print('Saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
