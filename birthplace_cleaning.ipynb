{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birthplace (fødested) cleaning\n",
    "Here I inted to clean the birthplace (fødested) field in the data. Notice that the pre-processing scripts are in city_unification (conversion to UTF8, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries for data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Importing libraries for system management\n",
    "import os\n",
    "\n",
    "#Distance computation\n",
    "from pyjarowinkler import distance\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading DigDag\n",
    "and expanding with variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function adds extra rows in the dataframe *dd* for the \n",
    "# places where *word* exists and replaces it with *replacement*\n",
    "def adding_extra_rows(dd, word, replacement):\n",
    "    dd['special'] = dd['simplename'].apply(lambda x: word in x) \n",
    "    dd['simplename2'] = dd.simplename.apply(lambda x: x.replace(word,replacement))\n",
    "    return pd.concat([dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename']],dd[dd['special']][['navn', 'enhedid', 'enhedtype', 'art', 'simplename2']].rename(columns={'simplename2':'simplename'})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>navn</th>\n",
       "      <th>enhedid</th>\n",
       "      <th>enhedtype</th>\n",
       "      <th>art</th>\n",
       "      <th>simplename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kronborg Amt</td>\n",
       "      <td>118765</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>kronborg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Præstø Amt</td>\n",
       "      <td>118791</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>præstø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bornholms Amt</td>\n",
       "      <td>118792</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>bornholms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Svendborg Amt</td>\n",
       "      <td>118813</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>svendborg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ålborg Amt</td>\n",
       "      <td>118819</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>ålborg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sorø Amt</td>\n",
       "      <td>118785</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>sorø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Århus Amt</td>\n",
       "      <td>118846</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>århus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hjørring Amt</td>\n",
       "      <td>118820</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>hjørring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bøvling Amt</td>\n",
       "      <td>118851</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>bøvling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vejle Amt</td>\n",
       "      <td>118849</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>vejle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dueholm Amt</td>\n",
       "      <td>118821</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>dueholm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ørum Amt</td>\n",
       "      <td>118824</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>ørum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thisted Amt</td>\n",
       "      <td>118826</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>thisted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Viborg Amt</td>\n",
       "      <td>118830</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>viborg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ringkøbing Amt</td>\n",
       "      <td>118853</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>ringkøbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Københavns Kommune</td>\n",
       "      <td>370923</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>københavns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Frederiksberg Kommune</td>\n",
       "      <td>370924</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>frederiksberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Fyns Amt</td>\n",
       "      <td>118739</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>fyns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sæbygård Amt</td>\n",
       "      <td>118773</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>sæbygård</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ribe Amt</td>\n",
       "      <td>118855</td>\n",
       "      <td>11</td>\n",
       "      <td>amt</td>\n",
       "      <td>ribe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     navn enhedid enhedtype  art     simplename\n",
       "0            Kronborg Amt  118765        11  amt       kronborg\n",
       "1              Præstø Amt  118791        11  amt         præstø\n",
       "2           Bornholms Amt  118792        11  amt      bornholms\n",
       "3           Svendborg Amt  118813        11  amt      svendborg\n",
       "4              Ålborg Amt  118819        11  amt         ålborg\n",
       "5                Sorø Amt  118785        11  amt           sorø\n",
       "6               Århus Amt  118846        11  amt          århus\n",
       "7            Hjørring Amt  118820        11  amt       hjørring\n",
       "8             Bøvling Amt  118851        11  amt        bøvling\n",
       "9               Vejle Amt  118849        11  amt          vejle\n",
       "10            Dueholm Amt  118821        11  amt        dueholm\n",
       "11               Ørum Amt  118824        11  amt           ørum\n",
       "12            Thisted Amt  118826        11  amt        thisted\n",
       "13             Viborg Amt  118830        11  amt         viborg\n",
       "14         Ringkøbing Amt  118853        11  amt     ringkøbing\n",
       "15     Københavns Kommune  370923        11  amt     københavns\n",
       "16  Frederiksberg Kommune  370924        11  amt  frederiksberg\n",
       "17               Fyns Amt  118739        11  amt           fyns\n",
       "18           Sæbygård Amt  118773        11  amt       sæbygård\n",
       "19               Ribe Amt  118855        11  amt           ribe"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The digdag seems to have duplicates which I do not understand why\n",
    "dd = pd.read_csv('../../city_dump/data/rl_places_digdag_v1.txt', sep = '\\t', encoding='utf-16', dtype=str)\n",
    "dd_org = dd.copy()\n",
    "\n",
    "#Getting the Købstad and putting it to the original data\n",
    "dd2 = pd.read_csv('../../city_dump/data/koebstad.csv', sep = ';', encoding='utf-8', dtype=str)\n",
    "dd2['enhedtype'] = dd2['art'] \n",
    "dd = pd.concat([dd, dd2[~dd2.isna()][['navn','enhedid','enhedtype','art','simplename']]])\n",
    "dd['simplename'] = dd['simplename'].astype(str)\n",
    "\n",
    "\n",
    "#Moving the Købstad to the right column\n",
    "dd['art'] = np.where(dd['enhedtype'] == 'Købstad', ['Købstadskommune']*len(dd), dd['art'])\n",
    "\n",
    "#adding a conversion of special characters to latin letters (å -> aa, ø -> oe, æ -> ae) and adding them to the reference list\n",
    "dd = adding_extra_rows(dd, 'å', 'aa')\n",
    "dd = adding_extra_rows(dd, 'ø', 'oe')\n",
    "dd = adding_extra_rows(dd, 'æ', 'ae')\n",
    "\n",
    "#Preparing the names to append them at the simple name\n",
    "dd['art'] = dd['art'].apply({'Amt':'amt', 'Sogn':'sogn','Købstadskommune':'købstad', 'Geografisk Herred':'herred', 'Processing':''}.get)\n",
    "\n",
    "#Adding a duplicated list of their unit type to increase matches\n",
    "dd['simplename2'] = dd.apply(lambda row: str(row['simplename'])+' '+str(row['art']), axis=1)\n",
    "dd = pd.concat([dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename']],dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename2']].rename(columns={'simplename2':'simplename'})])\n",
    "\n",
    "#Adding the plural købstæder as well\n",
    "dd = adding_extra_rows(dd, 'købstad', 'købstæder')\n",
    "\n",
    "#Adding spaces instead of hyphens (new rows)\n",
    "dd = adding_extra_rows(dd, '-', ' ')\n",
    "\n",
    "#Removing spaces (new rows)\n",
    "dd = adding_extra_rows(dd, ' ', '')\n",
    "\n",
    "#Adding bysogn and landsogn (new rows)\n",
    "dd = adding_extra_rows(dd, 'sogn', 'bysogn')\n",
    "dd = adding_extra_rows(dd, 'sogn', 'landsogn')\n",
    "\n",
    "#adding rows where the last letter's \"e\" will be removed\n",
    "dd['special'] = dd['simplename'].str[-1] == 'e'\n",
    "dd['simplename2'] = dd['simplename'].str[:-1]\n",
    "dd = pd.concat([dd[['navn', 'enhedid', 'enhedtype', 'art', 'simplename']],dd[dd['special']][['navn', 'enhedid', 'enhedtype', 'art', 'simplename2']].rename(columns={'simplename2':'simplename'})])\n",
    "\n",
    "#Dropping duplicates from digdag\n",
    "#print(dd[dd.duplicated('simplename', keep=False)].sort_values('simplename').head(30))\n",
    "dd.drop_duplicates(['art','simplename'], keep='first', inplace=True)\n",
    "\n",
    "#Drop 'Non' from the list, this may be one of my artifacts...\n",
    "dd = dd[(dd.simplename.apply(lambda x: not 'Non' in x))]\n",
    "\n",
    "# TODO take into consideration that the same city name can be in varios geographical areas...\n",
    "# It should be matched with herred to increase accuracy for each record. However, right now we're only cleaning\n",
    "# not sure how pressing the issue is...\n",
    "nr_sogn = len(dd_org[dd_org.art=='Sogn'].simplename.unique()) \n",
    "dd.head(20)\n",
    "\n",
    "#TODO: Right now I can map things to amt for example which should not be the case...\n",
    "#dd =dd[dd.art='sogn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length original:\t 2519 \n",
      "length modified:\t 27097 \n",
      "unique sogn keys:\t 1924\n"
     ]
    }
   ],
   "source": [
    "print('length original:\\t', len(dd_org), '\\nlength modified:\\t', len(dd), '\\nunique sogn keys:\\t', nr_sogn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the df from the $ separated file (for some reason pandas has problems with random lines)\n",
    "def get_df(f_path):\n",
    "    r= []\n",
    "    columns = []\n",
    "    with open(f_path) as f:\n",
    "        first = True\n",
    "        for line in f:\n",
    "            line = line.rstrip().split('$')\n",
    "            if first:\n",
    "                length = len(line)\n",
    "                columns = line\n",
    "                first=False\n",
    "            else:\n",
    "                r.append(line[:length])\n",
    "                \n",
    "    return pd.DataFrame(data=r, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name cleaning function\n",
    "def name_cleaner(s, working_column):\n",
    "    try:\n",
    "        o =s[working_column].lower().rstrip().replace('  ', ' ')\n",
    "        #if it says \"her i sogn\", I get the value for \"Sogn\" and put it there\n",
    "        if ('heri sogn' in o or 'her i sogn' in o) and working_column != 'Sogne':\n",
    "            return name_cleaner(s, 'Sogne')\n",
    "        return o\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files to work (with the 1901 collapsed)\n",
    "working_data = ['ft1845_LL.txt',\n",
    " 'ft1850_LL.txt',\n",
    " 'ft1860_LL.txt',\n",
    " 'ft1880_LL.txt',\n",
    " 'ft1885_LL.txt',\n",
    " 'ft1901_LL.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft1845_LL.txt\n",
      "1415056\n",
      "ft1850_LL.txt\n"
     ]
    }
   ],
   "source": [
    "_path = '../../city_dump/data/utf8/'\n",
    "focus_column = 'fødested'\n",
    "\n",
    "#This is where I save the counts and then group it together\n",
    "out_list = []\n",
    "\n",
    "#where I save the missmatches and file year\n",
    "missmatches = []\n",
    "total_records = 0\n",
    "\n",
    "for f in working_data:\n",
    "    \n",
    "    print(f)\n",
    "    #Loading the data\n",
    "    df = get_df(_path+f)\n",
    "    \n",
    "    #Dropping empty rows due to the \"mal-conversion\" to utf8... The empty rows do contain data originally\n",
    "    #TODO: Fix the conversion thing (ask Barbara to provide me the UTF-8...)\n",
    "    df = df[~df[focus_column].isna() & (df[focus_column] !='')]\n",
    "    print(len(df))\n",
    "    total_records = total_records + len(df)\n",
    "    \n",
    "    #Preforming the name cleaning, be aware that I replace \"her i sogn\" in there\n",
    "    df[focus_column+'_data'] = df.apply(lambda row: name_cleaner(row, focus_column), axis=1)\n",
    "    \n",
    "    #Reducing Dimensionality and saving in RAM\n",
    "    out = df.groupby(focus_column+'_data').size().reset_index(name = 'counts')\n",
    "    out['year'] = f[2:6]\n",
    "    out_list.append(out)\n",
    "\n",
    "    \n",
    "out_list = pd.concat(out_list, sort=False)\n",
    "print('Done! :D')\n",
    "\n",
    "out_list.sort_values('counts',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting it nicely, on a more horitzontal table\n",
    "out_list = out_list.pivot(focus_column+'_data', 'year', 'counts').fillna(0).astype(int).reset_index().reset_index(drop=True)\n",
    "out_list = pd.DataFrame(out_list.values, columns=out_list.columns.tolist()) #Getting rid of the wrong index\n",
    "out_list['total'] = out_list.apply(lambda row: row['1845']+row['1850']+row['1860']+row['1880']+row['1885']+row['1901'],axis=1)\n",
    "out_list.sort_values('total', ascending=False, inplace=True)\n",
    "out_list.total.sum() #Around the 14k first places compule 80% of the people\n",
    "out_list.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice that I only match for Sogn\n",
    "out = out_list.merge(dd[['navn','simplename', 'art', 'enhedid']], left_on=focus_column+'_data', right_on='simplename', how='left')\n",
    "\n",
    "#Those are the missmatch\n",
    "miss = out[out.enhedid.isna()]\n",
    "miss.head()\n",
    "\n",
    "#Here are the match\n",
    "match = out[~out.enhedid.isna()]\n",
    "match.head()\n",
    "\n",
    "# Aggregating the match by enhedid and year\n",
    "agg_counts = match.groupby(['navn', 'enhedid', 'art'],as_index = False).agg({'1845':'sum', '1850':'sum','1860':'sum', '1880':'sum','1885':'sum', '1901':'sum'})\n",
    "\n",
    "# Grouping all enhedid to see how many different versions of the same Sogne we have in the data\n",
    "diff_counts = match.groupby(['navn','enhedid'])[focus_column+'_data'].apply(set).reset_index(name='extended_digdag') #if parenthesis is confusing add **.apply(lambda x: ', '.join(x))** before the reset index\n",
    "\n",
    "# Joining DFs and saving\n",
    "s = agg_counts.merge(diff_counts, on = ['navn','enhedid'])\n",
    "s.to_csv(_path+'../out/birthplaces_FT_uniquevalues_01.tsv', sep='\\t', index=False)\n",
    "print(len(s))\n",
    "s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Jaro distances for the missmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the potential first matches\n",
    "\n",
    "mm = miss[focus_column+'_data'].unique()\n",
    "sn = dd_org.simplename.unique() # Using the short version of dig dag to make it much faster (>x10)\n",
    "\n",
    "distance_jaro = np.zeros((len(sn),len(mm)))\n",
    "\n",
    "def compute_jaro(a,b):\n",
    "    try:\n",
    "        return distance.get_jaro_distance(a, b)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "for i in range(len(sn)):\n",
    "    if i%100 == 0: print(i, 'out of' , len(sn))\n",
    "        \n",
    "    #Computing jaro only\n",
    "    distance_jaro[i] = miss[focus_column+'_data'].apply(lambda x: compute_jaro(sn[i], x)).values\n",
    "    \n",
    "        \n",
    "#Index is the reference names found in digdag the columns the original data\n",
    "distance_jaro = pd.DataFrame(data = distance_jaro, columns = mm, index = sn)\n",
    "print(distance_jaro.shape, 'len mm,sn:', len(mm), len(sn)) #(26428, 1237)\n",
    "distance_jaro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_jaro.to_csv(_path+'../out/jaro_birthplaces.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the best match with a minimum threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.9\n",
    "\n",
    "#Getting the highest match for each Sogne and filtering threshold\n",
    "a = distance_jaro.idxmax().reset_index(name = 'potential_match')\n",
    "b = distance_jaro.max().reset_index(name='jaro')\n",
    "dfmap = a.merge(b, on='index').rename(columns={'index':'data_name'})\n",
    "dfmap.to_csv(_path+'../out/mapping_birthplaces.tsv', sep='\\t', index=False)\n",
    "dfmap = dfmap[dfmap.jaro.astype(float) >= THRESHOLD]\n",
    "dfmap.head()\n",
    "\n",
    "#Getting the mapped info into DigDag v2 (not really the v2)\n",
    "dd2 = dfmap.merge(dd[['navn','simplename', 'art', 'enhedid']].sort_values('art', ascending=False).drop_duplicates('simplename', keep='first'), left_on='potential_match', right_on='simplename')\n",
    "dd2['simplename'] = dd2['data_name']\n",
    "dd2 = dd2[['navn','enhedid','art','simplename']]\n",
    "dd2 = pd.concat([dd2[['navn','simplename', 'art', 'enhedid']], dd], sort=False) #putting together\n",
    "dd2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting again with the merges\n",
    "out = out_list.merge(dd2, left_on=focus_column+'_data', right_on='simplename', how='left')\n",
    "\n",
    "#Those are the missmatch\n",
    "miss = out[out.enhedid.isna()]\n",
    "miss.head()\n",
    "\n",
    "#Here are the match\n",
    "match = out[~out.enhedid.isna()]\n",
    "match.head()\n",
    "\n",
    "# Aggregating the match by enhedid and year\n",
    "agg_counts = match.groupby(['navn', 'enhedid', 'art'],as_index = False).agg({'1845':'sum', '1850':'sum','1860':'sum', '1880':'sum','1885':'sum', '1901':'sum'})\n",
    "\n",
    "# Grouping all enhedid to see how many different versions of the same Sogne we have in the data\n",
    "diff_counts = match.groupby(['navn','enhedid'])[focus_column+'_data'].apply(set).reset_index(name='extended_digdag') #if parenthesis is confusing add **.apply(lambda x: ', '.join(x))** before the reset index\n",
    "\n",
    "# Joining DFs and saving\n",
    "s = agg_counts.merge(diff_counts, on = ['navn','enhedid'])\n",
    "s.to_csv(_path+'../out/birthplaces_FT_jaro0.9_01.tsv', sep='\\t', index=False)\n",
    "print(len(s))\n",
    "s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting counts for other possible matches (broke here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best possible matches\n",
    "\n",
    "concatenate = []\n",
    "i=0\n",
    "for col in distance_jaro.columns:\n",
    "    i = i+1\n",
    "    if i % 100 == 0: print(i, 'out of', len(distance_jaro.columns))\n",
    "    aux = distance_jaro.nlargest(5, col)[[col]]\n",
    "    aux.columns = ['score']\n",
    "    aux['original'] = col\n",
    "    concatenate.append(aux)\n",
    "\n",
    "concatenate = pd.concat(concatenate, sort=False)\n",
    "concatenate = concatenate.reset_index()\n",
    "concatenate.columns = ['potential_match','score','original']\n",
    "concatenate[['original','potential_match','score']][~concatenate.original.isin(concatenate[(concatenate.score >= THRESHOLD) & ~concatenate.original.isna()].original.unique())].to_csv(_path+'../out/birthplaces_possible_matches_jaro_01.tsv', index=False, sep='\\t')\n",
    "\n",
    "concatenate = pd.read_csv(_path+'../out/birthplaces_possible_matches_jaro_01.tsv',  sep='\\t')\n",
    "concatenate.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the digDag info for the matches but also for the non matches\n",
    "\n",
    "m = concatenate.merge(dd2[['navn','simplename', 'art', 'enhedid']].sort_values('art', ascending=False).drop_duplicates('simplename',keep='first'), left_on='potential_match', right_on='simplename', how='inner')\n",
    "m = m[['original','potential_match','score','navn','art','enhedid', 'simplename']]\n",
    "\n",
    "w = dd2.sort_values('art', ascending=False).drop_duplicates('simplename',keep='first')\n",
    "w['potential_match'] = w['simplename']\n",
    "w['original'] = w['simplename']\n",
    "w['score'] = 1\n",
    "\n",
    "m = pd.concat([w, m], sort=True)\n",
    "\n",
    "\n",
    "#Putting the things into place\n",
    "m['original'] = np.where(m.original.isna(), m['simplename'], m['original'])\n",
    "\n",
    "print(len(concatenate), len(m))\n",
    "m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the counts for the original plus the counts for the potential matches in DigDag\n",
    "#Warning: Potential matches may not exist in the data...\n",
    "out = m.merge(out_list, left_on='original', right_on='Sogne_data', how='left').merge(out_list, left_on='potential_match', right_on=focus_column+'_data', how='left').fillna(0)\n",
    "for y in ['1845', '1850', '1860', '1880', '1885', '1901']:\n",
    "    out[y] = out[y+'_x'] + out[y+'_y']\n",
    "out = out[['original', 'potential_match', 'score', 'navn', 'art', 'enhedid', '1845', '1850', '1860', '1880', '1885', '1901']].sort_values('original')\n",
    "out.to_csv(_path+'../out/birthplaces_possible_matches_jaro_01.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done! :D')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
